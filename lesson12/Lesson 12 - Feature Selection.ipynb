{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A New Enron Feature Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snewman/.Envs/sandbox/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import reader\n",
    "import poi_emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getToFromStrings(f):\n",
    "    '''\n",
    "    The imported reader.py file contains functions that we've created to help\n",
    "    parse e-mails from the corpus. .getAddresses() reads in the opening lines\n",
    "    of an e-mail to find the To: From: and CC: strings, while the\n",
    "    .parseAddresses() line takes each string and extracts the e-mail addresses\n",
    "    as a list.\n",
    "    '''\n",
    "    f.seek(0)\n",
    "    to_string, from_string, cc_string   = reader.getAddresses(f)\n",
    "    to_emails   = reader.parseAddresses( to_string )\n",
    "    from_emails = reader.parseAddresses( from_string )\n",
    "    cc_emails   = reader.parseAddresses( cc_string )\n",
    "\n",
    "    return to_emails, from_emails, cc_emails\n",
    "\n",
    "\n",
    "def poiFlagEmail(f):\n",
    "    \"\"\" given an email file f,\n",
    "        return a trio of booleans for whether that email is\n",
    "        to, from, or cc'ing a poi \"\"\"\n",
    "\n",
    "    to_emails, from_emails, cc_emails = getToFromStrings(f)\n",
    "\n",
    "    ### poi_emails.poiEmails() returns a list of all POIs' email addresses.\n",
    "    poi_email_list = poi_emails.poiEmails()\n",
    "\n",
    "    to_poi = False\n",
    "    from_poi = False\n",
    "    cc_poi   = False\n",
    "\n",
    "    ### to_poi and cc_poi are boolean variables which flag whether the email\n",
    "    ### under inspection is addressed to a POI, or if a POI is in cc,\n",
    "    ### respectively. You don't have to change this code at all.\n",
    "\n",
    "    ### There can be many \"to\" emails, but only one \"from\", so the\n",
    "    ### \"to\" processing needs to be a little more complicated\n",
    "    if to_emails:\n",
    "        ctr = 0\n",
    "        while not to_poi and ctr < len(to_emails):\n",
    "            if to_emails[ctr] in poi_email_list:\n",
    "                to_poi = True\n",
    "            ctr += 1\n",
    "\n",
    "    if cc_emails:\n",
    "        ctr = 0\n",
    "        while not cc_poi and ctr < len(cc_emails):\n",
    "            if cc_emails[ctr] in poi_email_list:\n",
    "                cc_poi = True\n",
    "            ctr += 1\n",
    "\n",
    "    #################################\n",
    "    ######## your code below ########\n",
    "    ### set from_poi to True if #####\n",
    "    ### the email is from a POI #####\n",
    "    #################################\n",
    "    if from_emails:\n",
    "        ctr = 0\n",
    "        while not from_poi and ctr < len(from_emails):\n",
    "            if from_emails[ctr] in poi_email_list:\n",
    "                from_poi = True\n",
    "            ctr += 1\n",
    "\n",
    "    #################################\n",
    "    return to_poi, from_poi, cc_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Visualizing Your New Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.0470879801735\n",
      "0.0344827586207\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0130890052356\n",
      "0.0\n",
      "\n",
      "0.0306220095694\n",
      "0.65625\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0246284501062\n",
      "0.541666666667\n",
      "\n",
      "0.0187234042553\n",
      "0.0139794967381\n",
      "\n",
      "0.0492730210016\n",
      "0.216216216216\n",
      "\n",
      "0.078125\n",
      "1.0\n",
      "\n",
      "0.108108108108\n",
      "0.0\n",
      "\n",
      "0.010101010101\n",
      "0.142857142857\n",
      "\n",
      "0.013978088402\n",
      "0.342105263158\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.136518771331\n",
      "0.275\n",
      "\n",
      "0.0882352941176\n",
      "0.0\n",
      "\n",
      "0.0968992248062\n",
      "0.339285714286\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0253353204173\n",
      "0.0\n",
      "\n",
      "0.0302227573751\n",
      "0.112268518519\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0291834833903\n",
      "0.0309585975382\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0104438642298\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.014312383323\n",
      "0.2\n",
      "\n",
      "0.0263554216867\n",
      "0.585365853659\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0196855775803\n",
      "0.0888786553074\n",
      "\n",
      "0.00698080279232\n",
      "0.368421052632\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.030303030303\n",
      "0.037037037037\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0105042016807\n",
      "0.0\n",
      "\n",
      "0.031746031746\n",
      "0.0\n",
      "\n",
      "0.0174459176553\n",
      "0.0093023255814\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0237420269313\n",
      "0.222222222222\n",
      "\n",
      "0.072737291638\n",
      "0.158994197292\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0689045936396\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.00488481087861\n",
      "0.0534979423868\n",
      "\n",
      "0.00878569187324\n",
      "0.25\n",
      "\n",
      "0.0577777777778\n",
      "0.358974358974\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.16106442577\n",
      "0.0540540540541\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0309021432132\n",
      "0.0431654676259\n",
      "\n",
      "0.0276359267047\n",
      "0.0707512764406\n",
      "\n",
      "0.0\n",
      "0.0625\n",
      "\n",
      "0.0213385063046\n",
      "0.19843597263\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0287853966768\n",
      "0.444444444444\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0168918918919\n",
      "0.288461538462\n",
      "\n",
      "0.0604026845638\n",
      "0.0769230769231\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0298165137615\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0448989773011\n",
      "0.0314270994333\n",
      "\n",
      "0.0753498385361\n",
      "0.555555555556\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.00106837606838\n",
      "0.5\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.122082585278\n",
      "0.221719457014\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0285320986109\n",
      "0.223140495868\n",
      "\n",
      "0.117256637168\n",
      "0.25\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.076597382602\n",
      "0.173611111111\n",
      "\n",
      "0.046408839779\n",
      "0.222222222222\n",
      "\n",
      "0.00889950075971\n",
      "0.0119014476615\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.121212121212\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0242624758754\n",
      "0.277777777778\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0636215334421\n",
      "0.72\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.00779510022272\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0109769484083\n",
      "0.0572569906791\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.055\n",
      "0.233333333333\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0161957270848\n",
      "0.0296127562642\n",
      "\n",
      "0.00765306122449\n",
      "0.0441176470588\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.00874831763122\n",
      "0.0243902439024\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0486787204451\n",
      "0.147058823529\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0344827586207\n",
      "\n",
      "0.0929487179487\n",
      "0.466666666667\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.00501824817518\n",
      "0.0444444444444\n",
      "\n",
      "0.0478468899522\n",
      "0.613636363636\n",
      "\n",
      "0.0568181818182\n",
      "0.214285714286\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.118584758942\n",
      "0.222222222222\n",
      "\n",
      "0.0293443374599\n",
      "0.02200220022\n",
      "\n",
      "0.073893129771\n",
      "0.285714285714\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0132125330313\n",
      "0.0358152686145\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0239316239316\n",
      "0.493333333333\n",
      "\n",
      "0.0223251895535\n",
      "0.245901639344\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0564516129032\n",
      "0.263157894737\n",
      "\n",
      "0.0217391304348\n",
      "0.578947368421\n",
      "\n",
      "0.00525624178712\n",
      "0.0\n",
      "\n",
      "0.0442804428044\n",
      "0.0674264007597\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.0306553911205\n",
      "0.244897959184\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "\n",
      "0.217341040462\n",
      "0.5\n",
      "\n",
      "0.0595647193585\n",
      "0.375\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from get_data import getData\n",
    "\n",
    "def computeFraction( poi_messages, all_messages ):\n",
    "    \"\"\" given a number messages to/from POI (numerator) \n",
    "        and number of all messages to/from a person (denominator),\n",
    "        return the fraction of messages to/from that person\n",
    "        that are from/to a POI\n",
    "    \"\"\"\n",
    "    if all_messages == 'NaN' or poi_messages == 'NaN':\n",
    "        fraction = 0.0\n",
    "    else:\n",
    "        fraction = float(poi_messages) / float(all_messages)\n",
    "    \n",
    "    return fraction\n",
    "\n",
    "data_dict = getData() \n",
    "submit_dict = {}\n",
    "\n",
    "for name in data_dict:\n",
    "\n",
    "    data_point = data_dict[name]\n",
    "\n",
    "    print\n",
    "    from_poi_to_this_person = data_point[\"from_poi_to_this_person\"]\n",
    "    to_messages = data_point[\"to_messages\"]\n",
    "    fraction_from_poi = computeFraction( from_poi_to_this_person, to_messages )\n",
    "    print fraction_from_poi\n",
    "    data_point[\"fraction_from_poi\"] = fraction_from_poi\n",
    "\n",
    "\n",
    "    from_this_person_to_poi = data_point[\"from_this_person_to_poi\"]\n",
    "    from_messages = data_point[\"from_messages\"]\n",
    "    fraction_to_poi = computeFraction( from_this_person_to_poi, from_messages )\n",
    "    print fraction_to_poi\n",
    "    submit_dict[name]={\"from_poi_to_this_person\":fraction_from_poi,\n",
    "                       \"from_this_person_to_poi\":fraction_to_poi}\n",
    "\n",
    "    data_point[\"fraction_to_poi\"] = fraction_to_poi\n",
    "    \n",
    "    \n",
    "#####################\n",
    "\n",
    "def submitDict():\n",
    "    return submit_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of Features and Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is `find_signature.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(42)\n",
    "\n",
    "words_file = \"your_word_data.pkl\" \n",
    "authors_file = \"your_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "   cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train = labels_train[:150]\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print len(features_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy of Your Overfit Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.947667804323\n"
     ]
    }
   ],
   "source": [
    "print clf.score(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify the Most Powerful Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76470588235294124]\n",
      "33614\n"
     ]
    }
   ],
   "source": [
    "importances = [i for i in clf.feature_importances_ if i >= 0.2]\n",
    "\n",
    "print importances\n",
    "print list(clf.feature_importances_).index(importances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use TfIdf to Get the Most Important Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshacklensf\n"
     ]
    }
   ],
   "source": [
    "print vectorizer.get_feature_names()[33614]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove, Repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from_sara  = open('from_sara.txt', 'r')\n",
    "from_chris = open('from_chris.txt', 'r')\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "def parseOutText(f):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    f.seek(0) # go back to beginning of file\n",
    "    all_text = f.read()\n",
    "    content = all_text.split('X-FileName:') # split off metadata\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "        words = text_string.split()\n",
    "        stemmed_words = [stemmer.stem(w) for w in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "    \n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        #temp_counter += 1\n",
    "        if temp_counter < 200:\n",
    "            path = os.path.join('../../enron_email', path[:-1])\n",
    "            email = open(path, \"r\")\n",
    "            words = parseOutText(email)\n",
    "            words = words.replace('sara', '')\n",
    "            words = words.replace('shackleton', '')\n",
    "            words = words.replace('chris', '')\n",
    "            words = words.replace('germani', '')\n",
    "            words = words.replace('sshacklensf', '')\n",
    "            word_data.append(words)\n",
    "            if name == \"sara\":\n",
    "                from_data.append(\"0\")\n",
    "            else:\n",
    "                from_data.append(\"1\")\n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_updated_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_updated_email_authors.pkl\", \"w\") )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun `find_signature.py`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66666666666666674]\n",
      "cgermannsf\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(42)\n",
    "\n",
    "words_file = \"your_updated_word_data.pkl\" \n",
    "authors_file = \"your_updated_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "   cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train = labels_train[:150]\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "importances = [i for i in clf.feature_importances_ if i > 0.2]\n",
    "\n",
    "print importances\n",
    "idx = list(clf.feature_importances_).index(importances[0])\n",
    "print vectorizer.get_feature_names()[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Important Features Again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emails processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import re\n",
    "import string\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "from_sara  = open('from_sara.txt', 'r')\n",
    "from_chris = open('from_chris.txt', 'r')\n",
    "\n",
    "from_data = []\n",
    "word_data = []\n",
    "\n",
    "def parseOutText(f):\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    f.seek(0) # go back to beginning of file\n",
    "    all_text = f.read()\n",
    "    content = all_text.split('X-FileName:') # split off metadata\n",
    "    if len(content) > 1:\n",
    "        ### remove punctuation\n",
    "        text_string = content[1].translate(string.maketrans(\"\", \"\"), string.punctuation)\n",
    "        words = text_string.split()\n",
    "        stemmed_words = [stemmer.stem(w) for w in words]\n",
    "    return ' '.join(stemmed_words)\n",
    "    \n",
    "### temp_counter is a way to speed up the development--there are\n",
    "### thousands of emails from Sara and Chris, so running over all of them\n",
    "### can take a long time\n",
    "### temp_counter helps you only look at the first 200 emails in the list so you\n",
    "### can iterate your modifications quicker\n",
    "temp_counter = 0\n",
    "\n",
    "for name, from_person in [(\"sara\", from_sara), (\"chris\", from_chris)]:\n",
    "    for path in from_person:\n",
    "        ### only look at first 200 emails when developing\n",
    "        ### once everything is working, remove this line to run over full dataset\n",
    "        #temp_counter += 1\n",
    "        if temp_counter < 200:\n",
    "            path = os.path.join('../../enron_email', path[:-1])\n",
    "            email = open(path, \"r\")\n",
    "            words = parseOutText(email)\n",
    "            words = words.replace('sara', '')\n",
    "            words = words.replace('shackleton', '')\n",
    "            words = words.replace('chris', '')\n",
    "            words = words.replace('germani', '')\n",
    "            words = words.replace('sshacklensf', '')\n",
    "            words = words.replace('cgermannsf', '')\n",
    "            word_data.append(words)\n",
    "            if name == \"sara\":\n",
    "                from_data.append(\"0\")\n",
    "            else:\n",
    "                from_data.append(\"1\")\n",
    "            email.close()\n",
    "\n",
    "print \"emails processed\"\n",
    "from_sara.close()\n",
    "from_chris.close()\n",
    "\n",
    "pickle.dump( word_data, open(\"your_updated2_word_data.pkl\", \"w\") )\n",
    "pickle.dump( from_data, open(\"your_updated2_email_authors.pkl\", \"w\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36363636363636365]\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "houectect\n"
     ]
    }
   ],
   "source": [
    "numpy.random.seed(42)\n",
    "\n",
    "words_file = \"your_updated2_word_data.pkl\" \n",
    "authors_file = \"your_updated2_email_authors.pkl\"\n",
    "word_data = pickle.load( open(words_file, \"r\"))\n",
    "authors = pickle.load( open(authors_file, \"r\") )\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "   cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "features_train = vectorizer.fit_transform(features_train)\n",
    "features_test = vectorizer.transform(features_test).toarray()\n",
    "\n",
    "features_train = features_train[:150].toarray()\n",
    "labels_train = labels_train[:150]\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "importances = [i for i in clf.feature_importances_ if i > 0.2]\n",
    "\n",
    "print importances\n",
    "print clf.feature_importances_\n",
    "idx = list(clf.feature_importances_).index(importances[0])\n",
    "print vectorizer.get_feature_names()[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.816837315131\n"
     ]
    }
   ],
   "source": [
    "print clf.score(features_test, labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
